<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="UTF-8">
	<title>ARTIST - A Real Time Improvisation System | CPSC 473 | Yale University</title>
	<link rel="stylesheet" href="css/home.css">
	<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"></script>
	<script src="js/index.js"></script>
	<script src="js/parallax.min.js"></script>
</head>
<body>

<div class="navbar">
	<section class="navbarHeader"><a href="/"><img id="logo" src="img/logo2-black.png"></a></section>
	<section class="mainNavbar">
		<a class="navlink scrollToDiv aboutLink" href="#" data-target="about">ABOUT</a>
    <a class="navlink scrollToDiv pricingLink" href="#" data-target="resources">RESOURCES</a>
    <a class="navlink" target="_blank" href="http://jasonsbrooks.github.io/ARTIST/documentation/">DOCUMENTATION</a>
    <a class="navlink scrollToDiv contactLink" href="#" data-target="team">THE TEAM</a>
	</section>
</div>
<div class="hero">
	<img id="large-logo" src="img/logo2-white-fulltext.png">
	<a class="aboutLink scrollToDiv" href="#" data-target="about"><div class="arrow animated bounce" id="arrow-down"></div></a>
</div>


<section id="about">
  <div id="about-title">
    <h1>ABOUT</h1>
  </div>
  <div id="about-content">
    <div class="letter">
		<p>As robots become more integrated in human life, their effectiveness will increasingly depend on their ability to seamlessly communicate with human counterparts. ARTIST aims to tackle this problem of human-robot communication in the domain of jazz improvisation. We present a system that can dynamically, appropriately, and creatively play with human jazz musicians.</p>

    <p>Music in general, especially jazz, lives within a series of constraints. For example, when selecting notes during improvisation, a player typically chooses within the chord and mode. Furthermore, with the goal of performing on a robotics platform, there are limitations on the sequences of notes and durations that can be played.</p>

    <p>DARPA has already shown interest in similar jazz improvisation systems with the expectation that examining this problem in a restricted domain will produce insights that can be applied to military robotics.  We hope that engineering ARTIST will not only result in the development of a novel jazz improvisation technology, but will also inform broader areas of human-robot communication for future research.</p>

    <p>ARTIST works in a six stage pipeline. First, large repositories of Musical Instrument Digital Interface (MIDI) files are mined from Internet sources, each of which contains a computational representation of the composed jazz song. The chord progressions are extracted from the various songs and a trigram model is used to extract frequent patterns. Via a novel algorithmic pipeline, jazz improvisation is generated for a given chord using a combination of a statistical trigram model and a genetic algorithm. Finally, the improvisation is played via a robotic performer through a set of generated position transformations.</p>

		
	</div>
  </div>
</section>

<section id="resources">
  <div id="resources-title">
    <h1>Resources</h1>
  </div>
  <div id="resources-content">
    <ul>
      <li><a class="form-link" target="_blank" href="https://github.com/jasonsbrooks/ARTIST/paper">Paper</a></li>
      <li><a class="form-link" target="_blank" href="https://github.com/jasonsbrooks/ARTIST">Github Repository</a></li>
      <li><a class="form-link" target="_blank" href="http://jasonsbrooks.github.io/ARTIST/documentation/">Documentation</a></li>
      <li><a class="form-link" href="http://jasonsbrooks.github.io/ARTIST/resources/databases.zip">Databases</a></li>
      <li><a class="form-link" href="http://jasonsbrooks.github.io/ARTIST/resources/models.zip">Trained N-Gram Models</a></li>
      <li><a class="form-link" href="http://jasonsbrooks.github.io/ARTIST/resources/samples.zip">Sample Generated MIDI Files</a></li>
      <li>Videos:</li>
      <li>Presentations:</li>
    </ul>
    <iframe src="//www.slideshare.net/slideshow/embed_code/key/DO636CSsLGN9BJ" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/secret/DO636CSsLGN9BJ" title="CPSC 473 Final Presentation" target="_blank">CPSC 473 Final Presentation</a> </strong> from <strong><a href="//www.slideshare.net/JasonBrooks32" target="_blank">Jason Brooks</a></strong> </div>
  </div>
  </div>
</section>





<section id="team">
  <div id="team-title">
    <h1>The Team</h1>
  </div>
  <div id="team-members">
    <p>Research on ARTIST was conducted at Yale University by three undergraduates as part of CPSC 473: Intelligent Robotics Lab. The work was advised by <a class="form-link" target="_blank" href="http://scazlab.yale.edu/people/brian-scassellati">Professor Brian Scassellati</a> in the <a class="form-link" target="_blank" href="http://scazlab.yale.edu">Scaz Lab</a>.</p>
    <div id="team-leadership">
      <article>
        <img src="img/jason.png" alt="" />
            <p class="person-name">Jason Brooks</p>
            <p class="person-title">DC '16</p>
      </article>
      <article>
        <img src="img/kevin.png" alt="" />
            <p class="person-name">Kevin Jiang</p>
            <p class="person-title">DC '16</p>
      </article>
      <article>
        <img src="img/charlie.png" alt="" />
            <p class="person-name">Charlie Proctor</p>
            <p class="person-title">TD '17</p>
      </article>
      <div class="clearer"></div>
    </div>
    <br>
    <div class="clearer"></div>
    <div id="team-contributions">
		<p>Jason primarily worked on hardware-related tasks, including the interaction between the learning algorithm and <a class="form-link" target="_blank" href="http://www.rethinkrobotics.com/baxter/">Baxter</a>, getting Baxter to play notes on the xylophone, and mallet design for the robot. He also built this site, was the resident music expert for early stage algorithm evaluation. Kevin built the genetic algorithm pipeline that is the final step in generating MIDI tracks and Baxter tracks. He constructed the MIDI parser module that enables storage of MIDI files in an actionable format for harmonic analysis and n-gram seeding. On the hardware side, Charlie wrote the training program for Baxter to allow it to play on any xylophone. He also wrote the harmonic analyzer, the ngram model, and helped piece together the machine learning pipeline.
		</p>
    </div>
  </div>
</section>

<section id="footer">
</section>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-61565238-1', 'auto');
  ga('send', 'pageview');

</script>

</body>
</html>